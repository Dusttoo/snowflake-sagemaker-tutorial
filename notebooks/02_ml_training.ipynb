{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animal Adoption Prediction Model Training\n",
    "This notebook trains a machine learning model to predict animal adoption outcomes using the insights from our data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"‚úÖ Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Get these values from terraform output\n",
    "import subprocess\n",
    "\n",
    "def get_terraform_output(output_name):\n",
    "    \"\"\"Helper function to get terraform output values\"\"\"\n",
    "    # Find terraform directory - check multiple possible locations\n",
    "    possible_paths = [\n",
    "        '../terraform',           # If running from notebooks/ subdirectory\n",
    "        './terraform',            # If running from project root\n",
    "        '../../terraform',        # If running from notebooks/subfolder\n",
    "    ]\n",
    "    \n",
    "    terraform_dir = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path) and os.path.isfile(os.path.join(path, 'main.tf')):\n",
    "            terraform_dir = path\n",
    "            break\n",
    "    \n",
    "    if not terraform_dir:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find terraform directory. Make sure you're running this notebook \"\n",
    "            \"from the project root or notebooks/ directory, and that terraform/ exists.\"\n",
    "        )\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['terraform', 'output', '-raw', output_name], \n",
    "        cwd=terraform_dir, \n",
    "        capture_output=True, \n",
    "        text=True\n",
    "    )\n",
    "    return result.stdout.strip()\n",
    "\n",
    "try:\n",
    "    # Get actual values from your deployed infrastructure\n",
    "    BUCKET_NAME = get_terraform_output('s3_bucket_name')\n",
    "    SAGEMAKER_ROLE = get_terraform_output('sagemaker_role_arn')\n",
    "    REGION = 'us-east-1'\n",
    "    \n",
    "    print(f\"Using bucket: {BUCKET_NAME}\")\n",
    "    print(f\"Using role: {SAGEMAKER_ROLE}\")\n",
    "    \n",
    "    # Initialize SageMaker session\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not get Terraform outputs: {e}\")\n",
    "    print(\"Using local configuration for testing\")\n",
    "    BUCKET_NAME = None\n",
    "    SAGEMAKER_ROLE = None\n",
    "    REGION = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"Load and prepare data from processed file, S3, or raw data\"\"\"\n",
    "    # Try multiple data sources in order of preference\n",
    "    \n",
    "    # 1. Try processed data from data exploration notebook\n",
    "    try:\n",
    "        df = pd.read_csv('../data/processed_animal_data.csv')\n",
    "        print(f\"‚úÖ Loaded processed data from file: {len(df):,} records\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"No processed data file found, trying other sources...\")\n",
    "    \n",
    "    # 2. Try raw data from local file\n",
    "    try:\n",
    "        print(\"Loading raw data from local file...\")\n",
    "        df = pd.read_csv('../data/austin_animal_outcomes.csv')\n",
    "        print(f\"‚úÖ Loaded raw data from file: {len(df):,} records\")\n",
    "        \n",
    "        # Add age in days if not present\n",
    "        if 'age_in_days' not in df.columns:\n",
    "            print(\"Converting age to days...\")\n",
    "            df['age_in_days'] = df['Age upon Outcome'].apply(parse_age_in_days)\n",
    "            \n",
    "        # Add time features\n",
    "        print(\"Adding time features...\")\n",
    "        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "        df['outcome_month'] = df['DateTime'].dt.month\n",
    "        df['outcome_hour'] = df['DateTime'].dt.hour\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"No local data file found, trying S3...\")\n",
    "    \n",
    "    # 3. Try S3 as last resort (if bucket is configured)\n",
    "    if BUCKET_NAME:\n",
    "        try:\n",
    "            print(f\"Attempting to load from S3 bucket: {BUCKET_NAME}\")\n",
    "            s3 = boto3.client('s3')\n",
    "            \n",
    "            # List available files in the bucket\n",
    "            try:\n",
    "                response = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix='raw/')\n",
    "                if 'Contents' in response:\n",
    "                    print(\"Available files in S3:\")\n",
    "                    for obj in response['Contents']:\n",
    "                        print(f\"  - {obj['Key']}\")\n",
    "                    \n",
    "                    # Try to find a CSV file\n",
    "                    csv_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.csv')]\n",
    "                    if csv_files:\n",
    "                        key = csv_files[0]  # Use the first CSV file found\n",
    "                        print(f\"Loading from S3: {key}\")\n",
    "                        obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "                        df = pd.read_csv(obj['Body'])\n",
    "                        print(f\"‚úÖ Loaded data from S3: {len(df):,} records\")\n",
    "                        \n",
    "                        # Add age in days if not present\n",
    "                        if 'age_in_days' not in df.columns:\n",
    "                            print(\"Converting age to days...\")\n",
    "                            df['age_in_days'] = df['Age upon Outcome'].apply(parse_age_in_days)\n",
    "                            \n",
    "                        # Add time features\n",
    "                        print(\"Adding time features...\")\n",
    "                        df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "                        df['outcome_month'] = df['DateTime'].dt.month\n",
    "                        df['outcome_hour'] = df['DateTime'].dt.hour\n",
    "                        \n",
    "                        return df\n",
    "                else:\n",
    "                    print(\"No files found in S3 bucket\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error listing S3 objects: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load from S3: {e}\")\n",
    "    else:\n",
    "        print(\"No S3 bucket configured, skipping S3 check\")\n",
    "    \n",
    "    # 4. If all else fails, provide instructions\n",
    "    raise FileNotFoundError(\n",
    "        \"‚ùå Could not find data in any location!\\n\"\n",
    "        \"Please ensure you have one of:\\n\"\n",
    "        \"1. Run the data exploration notebook first to create processed_animal_data.csv\\n\"\n",
    "        \"2. Place austin_animal_outcomes.csv in the ../data/ directory\\n\"\n",
    "        \"3. Upload data to your S3 bucket in the raw/ folder\"\n",
    "    )\n",
    "\n",
    "def parse_age_in_days(age_str):\n",
    "    \"\"\"Convert age strings to days\"\"\"\n",
    "    if pd.isna(age_str):\n",
    "        return np.nan\n",
    "    \n",
    "    age_str = str(age_str).lower().strip()\n",
    "    \n",
    "    try:\n",
    "        if 'year' in age_str:\n",
    "            return float(age_str.split()[0]) * 365\n",
    "        elif 'month' in age_str:\n",
    "            return float(age_str.split()[0]) * 30\n",
    "        elif 'week' in age_str:\n",
    "            return float(age_str.split()[0]) * 7\n",
    "        elif 'day' in age_str:\n",
    "            return float(age_str.split()[0])\n",
    "        else:\n",
    "            return np.nan\n",
    "    except (ValueError, IndexError):\n",
    "        return np.nan\n",
    "\n",
    "# Load the data\n",
    "print(\"üîç Looking for data sources...\")\n",
    "df = prepare_data()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "if 'Outcome Type' in df.columns:\n",
    "    print(f\"Outcome types: {df['Outcome Type'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(\"Available columns:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Data Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df):\n",
    "    \"\"\"Preprocess features for training\"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Define categorical columns that need special handling\n",
    "    categorical_columns = ['Animal Type', 'Sex upon Outcome', 'Breed', 'Color']\n",
    "    \n",
    "    # Process each categorical column\n",
    "    for col in categorical_columns:\n",
    "        if col in df_processed.columns:\n",
    "            # Fill missing values with 'unknown'\n",
    "            df_processed[col] = df_processed[col].fillna('unknown')\n",
    "            \n",
    "            # Create simplified column name for ML\n",
    "            simple_name = col.lower().replace(' ', '_').replace('upon_', '')\n",
    "            \n",
    "            # For breed, extract primary breed (first word)\n",
    "            if col == 'Breed':\n",
    "                df_processed['primary_breed'] = df_processed[col].str.split().str[0]\n",
    "                # Limit to top 20 breeds to prevent overfitting\n",
    "                top_breeds = df_processed['primary_breed'].value_counts().head(20).index\n",
    "                df_processed['primary_breed'] = df_processed['primary_breed'].apply(\n",
    "                    lambda x: x if x in top_breeds else 'other'\n",
    "                )\n",
    "            else:\n",
    "                df_processed[simple_name] = df_processed[col]\n",
    "                # Limit to top categories for other fields too\n",
    "                if col != 'Animal Type':  # Keep all animal types\n",
    "                    top_categories = df_processed[simple_name].value_counts().head(20).index\n",
    "                    df_processed[simple_name] = df_processed[simple_name].apply(\n",
    "                        lambda x: x if x in top_categories else 'other'\n",
    "                    )\n",
    "    \n",
    "    # Handle missing numerical values\n",
    "    if 'age_in_days' in df_processed.columns:\n",
    "        df_processed['age_in_days'] = df_processed['age_in_days'].fillna(\n",
    "            df_processed['age_in_days'].median()\n",
    "        )\n",
    "    \n",
    "    # Add time features if DateTime exists\n",
    "    if 'DateTime' in df_processed.columns:\n",
    "        df_processed['DateTime'] = pd.to_datetime(df_processed['DateTime'])\n",
    "        if 'outcome_month' not in df_processed.columns:\n",
    "            df_processed['outcome_month'] = df_processed['DateTime'].dt.month\n",
    "        if 'outcome_hour' not in df_processed.columns:\n",
    "            df_processed['outcome_hour'] = df_processed['DateTime'].dt.hour\n",
    "    \n",
    "    # Create binary target: 1 for Adoption, 0 for others\n",
    "    df_processed['adopted_label'] = (df_processed['Outcome Type'] == 'Adoption').astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing complete. Binary adoption rate: {df_processed['adopted_label'].mean():.3f}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess the data\n",
    "df_processed = preprocess_features(df)\n",
    "print(f\"Processed dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    \n",
    "    # Define features for training\n",
    "    feature_columns = ['animal_type', 'sex_outcome', 'age_in_days', 'primary_breed', 'color', 'outcome_month']\n",
    "    \n",
    "    # Check which features exist\n",
    "    available_features = [col for col in feature_columns if col in df_processed.columns]\n",
    "    print(f\"Available features: {available_features}\")\n",
    "    \n",
    "    # Use available features\n",
    "    X = df_processed[available_features]\n",
    "    y = df_processed['adopted_label']  # Target: 1 = adopted, 0 = not adopted\n",
    "    \n",
    "    # Split data for training and validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(f\"Training adoption rate: {y_train.mean():.3f}\")\n",
    "    print(f\"Test adoption rate: {y_test.mean():.3f}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(X_train, X_test):\n",
    "    \"\"\"Convert categorical text data to numbers for machine learning\"\"\"\n",
    "    # Create copies to avoid modifying originals\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "    \n",
    "    # Store encoders for later use\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for column in X_train_encoded.columns:\n",
    "        if X_train_encoded[column].dtype == 'object':\n",
    "            print(f\"Encoding {column}...\")\n",
    "            \n",
    "            # Create and fit encoder on training data\n",
    "            encoder = LabelEncoder()\n",
    "            X_train_encoded[column] = encoder.fit_transform(X_train_encoded[column].astype(str))\n",
    "            \n",
    "            # Apply to test data, handling unseen categories\n",
    "            test_values = X_test_encoded[column].astype(str)\n",
    "            # Map unseen categories to 'unknown' if it exists, otherwise to the most common class\n",
    "            unseen_mask = ~test_values.isin(encoder.classes_)\n",
    "            if unseen_mask.any():\n",
    "                print(f\"  Found {unseen_mask.sum()} unseen categories in {column}\")\n",
    "                if 'unknown' in encoder.classes_:\n",
    "                    test_values[unseen_mask] = 'unknown'\n",
    "                else:\n",
    "                    # Use most common class\n",
    "                    most_common = X_train[column].mode().iloc[0]\n",
    "                    test_values[unseen_mask] = most_common\n",
    "            \n",
    "            X_test_encoded[column] = encoder.transform(test_values)\n",
    "            label_encoders[column] = encoder\n",
    "            \n",
    "            print(f\"  {column}: {len(encoder.classes_)} unique values\")\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded, label_encoders\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded, X_test_encoded, label_encoders = encode_features(X_train, X_test)\n",
    "print(f\"\\n‚úÖ Feature encoding complete!\")\n",
    "print(f\"Training features shape: {X_train_encoded.shape}\")\n",
    "print(f\"Test features shape: {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train the Random Forest model and evaluate its performance\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Training Random Forest model...\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of adoption\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nüìä Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìã Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Not Adopted', 'Adopted']))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top 10 Most Important Features:\")\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance.head(10).plot(x='feature', y='importance', kind='bar')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = train_and_evaluate(X_train_encoded, X_test_encoded, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Model and Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifacts():\n",
    "    \"\"\"Save the trained model and encoders for later use\"\"\"\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = '../models/animal_adoption_model.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"‚úÖ Model saved to {model_path}\")\n",
    "    \n",
    "    # Save the label encoders\n",
    "    encoders_path = '../models/label_encoders.pkl'\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    print(f\"‚úÖ Encoders saved to {encoders_path}\")\n",
    "    \n",
    "    # Save feature names for reference\n",
    "    feature_info = {\n",
    "        'feature_names': list(X_train_encoded.columns),\n",
    "        'n_features': len(X_train_encoded.columns),\n",
    "        'target_name': 'adopted_label'\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('../models/model_info.json', 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "    print(f\"‚úÖ Model info saved to ../models/model_info.json\")\n",
    "    \n",
    "    print(f\"\\nüéØ Model training completed and artifacts saved!\")\n",
    "    \n",
    "    return model_path, encoders_path\n",
    "\n",
    "# Save all artifacts\n",
    "model_path, encoders_path = save_model_artifacts()\n",
    "\n",
    "print(\"\\nüì¶ Training Summary:\")\n",
    "print(f\"Model type: Random Forest Classifier\")\n",
    "print(f\"Features used: {len(X_train_encoded.columns)}\")\n",
    "print(f\"Training samples: {len(X_train_encoded)}\")\n",
    "print(f\"Test samples: {len(X_test_encoded)}\")\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Encoders saved to: {encoders_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Test Model Loading (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model to make sure it works\n",
    "print(\"üîç Testing saved model loading...\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_encoders = joblib.load(encoders_path)\n",
    "\n",
    "# Test prediction on a few samples\n",
    "test_predictions = loaded_model.predict(X_test_encoded[:5])\n",
    "test_probabilities = loaded_model.predict_proba(X_test_encoded[:5])[:, 1]\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Test predictions: {test_predictions}\")\n",
    "print(f\"Test probabilities: {test_probabilities.round(3)}\")\n",
    "print(f\"Actual values: {y_test.iloc[:5].values}\")\n",
    "\n",
    "print(\"\\nüéâ ML Training Pipeline Complete!\")\n",
    "print(\"Ready for model deployment to SageMaker.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
