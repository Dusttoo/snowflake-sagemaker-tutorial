{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animal Insights Data Exploration\n",
    "This notebook explores the Austin Animal Center dataset for insights that will inform our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Configuration\n",
    "Run this first to configure Jupyter for optimal data science experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic commands for better notebook experience\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configure pandas display options\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"✅ Jupyter configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries\n",
    "Import all required libraries with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Test each import individually to catch specific errors\n",
    "try:\n",
    "    import boto3\n",
    "    print(\"✅ boto3 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ boto3 import failed: {e}\")\n",
    "    print(\"Fix: pip install boto3\")\n",
    "\n",
    "# Set up plotting with error handling\n",
    "try:\n",
    "    # Try different seaborn styles (newer versions changed names)\n",
    "    try:\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        print(\"✅ Using seaborn-v0_8 style\")\n",
    "    except:\n",
    "        try:\n",
    "            plt.style.use('seaborn')\n",
    "            print(\"✅ Using seaborn style\") \n",
    "        except:\n",
    "            plt.style.use('default')\n",
    "            print(\"✅ Using default style\")\n",
    "    \n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Test matplotlib setup\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot([1, 2, 3], [1, 4, 2])\n",
    "    plt.title(\"✅ Matplotlib is working correctly!\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ All plotting libraries configured successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Plotting setup failed: {e}\")\n",
    "    print(\"Try running: pip install matplotlib seaborn\")\n",
    "\n",
    "print(\"✅ Cell 2 completed - libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration and Data Loading\n",
    "Set up AWS configuration and load data from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration Loading\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration with multiple fallback options\"\"\"\n",
    "    \n",
    "    # Option 1: Load from config.json (recommended)\n",
    "    config_file = Path('../config.json')\n",
    "    if config_file.exists():\n",
    "        with open(config_file) as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"Configuration loaded from {config_file}\")\n",
    "        return config\n",
    "    \n",
    "    # Option 2: Try environment variables\n",
    "    if os.environ.get('S3_BUCKET_NAME'):\n",
    "        config = {\n",
    "            's3_bucket_name': os.environ['S3_BUCKET_NAME'],\n",
    "            'aws_region': os.environ.get('AWS_REGION', 'us-east-1')\n",
    "        }\n",
    "        print(\"Configuration loaded from environment variables\")\n",
    "        return config\n",
    "    \n",
    "    # Option 3: Interactive input \n",
    "    print(\"Configuration not found. Let's set it up interactively.\")\n",
    "    print(\"(You can skip this by running: python config_generator.py)\")\n",
    "    \n",
    "    config = {}\n",
    "    bucket_name = input(\"Enter your S3 bucket name (from terraform output): \").strip()\n",
    "    config['s3_bucket_name'] = bucket_name\n",
    "    config['aws_region'] = 'us-east-1'\n",
    "    \n",
    "    # Save for next time\n",
    "    with open('../config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(\"Configuration saved to config.json for future use\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    config = load_config()\n",
    "    BUCKET_NAME = config['s3_bucket_name']\n",
    "    AWS_REGION = config.get('aws_region', 'us-east-1')\n",
    "    \n",
    "    print(f\"Using S3 bucket: {BUCKET_NAME}\")\n",
    "    print(f\"Using AWS region: {AWS_REGION}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Configuration cancelled. Please run this cell again when ready.\")\n",
    "    BUCKET_NAME = None\n",
    "    AWS_REGION = 'us-east-1'\n",
    "\n",
    "class AnimalDataExplorer:\n",
    "    \"\"\"Class to explore and analyze animal shelter data\"\"\"\n",
    "    \n",
    "    def __init__(self, s3_bucket=None, data_path=None):\n",
    "        self.data = None\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from S3 or local file\"\"\"\n",
    "        if self.s3_bucket:\n",
    "            try:\n",
    "                s3 = boto3.client('s3')\n",
    "                obj = s3.get_object(Bucket=self.s3_bucket, \n",
    "                                  Key='raw/austin_animal_outcomes.csv')\n",
    "                self.data = pd.read_csv(obj['Body'])\n",
    "                print(f\"Data loaded from S3: {len(self.data):,} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load from S3: {e}\")\n",
    "                print(\"Trying local file instead...\")\n",
    "                self.data = pd.read_csv('../data/austin_animal_outcomes.csv')\n",
    "                print(f\"Data loaded from local file: {len(self.data):,} records\")\n",
    "        else:\n",
    "            # Load from local file\n",
    "            self.data = pd.read_csv(self.data_path or '../data/austin_animal_outcomes.csv')\n",
    "            print(f\"Data loaded from local file: {len(self.data):,} records\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "# Initialize explorer with S3 bucket\n",
    "explorer = AnimalDataExplorer(s3_bucket=BUCKET_NAME)\n",
    "explorer.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Basic Data Information\n",
    "Examine the structure and quality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Overview\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Shape: {explorer.data.shape}\")\n",
    "print(f\"Memory usage: {explorer.data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "display(explorer.data.head())\n",
    "\n",
    "print(\"\\nColumn Information:\")\n",
    "explorer.data.info()\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = explorer.data.isnull().sum()\n",
    "missing_pct = (missing / len(explorer.data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Outcome Analysis\n",
    "Analyze the distribution of animal outcomes - our target variable for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outcome types and trends\n",
    "print(\"Outcome Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Outcome distribution\n",
    "outcome_counts = explorer.data['Outcome Type'].value_counts()\n",
    "print(\"Outcome Type Distribution:\")\n",
    "for outcome, count in outcome_counts.items():\n",
    "    pct = (count / len(explorer.data)) * 100\n",
    "    print(f\"  {outcome}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Plot outcome distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "outcome_counts.plot(kind='bar')\n",
    "plt.title('Outcome Type Distribution')\n",
    "plt.xlabel('Outcome Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "outcome_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Outcome Type Percentage')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Temporal Pattern Analysis\n",
    "Understand how outcomes vary by time of day, month, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns in the data\n",
    "print(\"Temporal Patterns\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Convert DateTime to pandas datetime with better error handling\n",
    "print(\"Converting DateTime column...\")\n",
    "try:\n",
    "    # First, let's see what the DateTime column looks like\n",
    "    print(\"Sample DateTime values:\")\n",
    "    print(explorer.data['DateTime'].head())\n",
    "    print(f\"DateTime column type: {explorer.data['DateTime'].dtype}\")\n",
    "    \n",
    "    # Convert to datetime with error handling\n",
    "    explorer.data['DateTime'] = pd.to_datetime(explorer.data['DateTime'], errors='coerce')\n",
    "    \n",
    "    # Check for any conversion failures\n",
    "    null_count = explorer.data['DateTime'].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"Warning: {null_count} DateTime values could not be converted and were set to NaT\")\n",
    "    \n",
    "    # Remove rows with invalid dates for temporal analysis\n",
    "    valid_dates_mask = explorer.data['DateTime'].notna()\n",
    "    temp_data = explorer.data[valid_dates_mask].copy()\n",
    "    print(f\"Using {len(temp_data):,} records with valid dates for temporal analysis\")\n",
    "    \n",
    "    # Extract time components\n",
    "    temp_data['Year'] = temp_data['DateTime'].dt.year\n",
    "    temp_data['Month'] = temp_data['DateTime'].dt.month\n",
    "    temp_data['Hour'] = temp_data['DateTime'].dt.hour\n",
    "    temp_data['DayOfWeek'] = temp_data['DateTime'].dt.day_name()\n",
    "    \n",
    "    print(\"✅ DateTime conversion successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error processing DateTime: {e}\")\n",
    "    print(\"Skipping temporal analysis\")\n",
    "    # Create a simple plot instead\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.text(0.5, 0.5, 'Temporal analysis skipped due to DateTime conversion issues', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)\n",
    "    plt.title('Temporal Analysis - Conversion Error')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    temp_data = None\n",
    "\n",
    "# Only proceed with temporal analysis if we have valid data\n",
    "if temp_data is not None and len(temp_data) > 0:\n",
    "    # Yearly trends\n",
    "    yearly_outcomes = temp_data.groupby(['Year', 'Outcome Type']).size().unstack(fill_value=0)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Yearly trends\n",
    "    plt.subplot(2, 2, 1)\n",
    "    yearly_outcomes.plot(kind='line', ax=plt.gca())\n",
    "    plt.title('Yearly Outcome Trends')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Monthly patterns\n",
    "    plt.subplot(2, 2, 2)\n",
    "    monthly_counts = temp_data['Month'].value_counts().sort_index()\n",
    "    monthly_counts.plot(kind='bar')\n",
    "    plt.title('Monthly Distribution')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Hourly patterns\n",
    "    plt.subplot(2, 2, 3)\n",
    "    hourly_counts = temp_data['Hour'].value_counts().sort_index()\n",
    "    hourly_counts.plot(kind='line', marker='o')\n",
    "    plt.title('Hourly Distribution')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Day of week patterns\n",
    "    plt.subplot(2, 2, 4)\n",
    "    dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dow_counts = temp_data['DayOfWeek'].value_counts().reindex(dow_order)\n",
    "    dow_counts.plot(kind='bar')\n",
    "    plt.title('Day of Week Distribution')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Update the main data with the converted DateTime\n",
    "    explorer.data['DateTime'] = pd.to_datetime(explorer.data['DateTime'], errors='coerce')\n",
    "    explorer.data.loc[valid_dates_mask, 'Year'] = temp_data['Year']\n",
    "    explorer.data.loc[valid_dates_mask, 'Month'] = temp_data['Month']\n",
    "    explorer.data.loc[valid_dates_mask, 'Hour'] = temp_data['Hour']\n",
    "    explorer.data.loc[valid_dates_mask, 'DayOfWeek'] = temp_data['DayOfWeek']\n",
    "    \n",
    "    print(f\"✅ Temporal analysis complete using {len(temp_data):,} valid records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Animal Characteristics Analysis\n",
    "Explore animal types, breeds, ages, and how they relate to outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze animal types, breeds, and characteristics\n",
    "print(\"Animal Characteristics\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Animal type distribution\n",
    "animal_types = explorer.data['Animal Type'].value_counts()\n",
    "print(\"Top Animal Types:\")\n",
    "for animal_type, count in animal_types.head(10).items():\n",
    "    pct = (count / len(explorer.data)) * 100\n",
    "    print(f\"  {animal_type}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Breed analysis\n",
    "print(\"\\nTop 10 Breeds:\")\n",
    "breed_counts = explorer.data['Breed'].value_counts()\n",
    "for breed, count in breed_counts.head(10).items():\n",
    "    pct = (count / len(explorer.data)) * 100\n",
    "    print(f\"  {breed}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Create age conversion function for analysis\n",
    "def convert_age_to_days(age_series):\n",
    "    \"\"\"Convert age strings to days\"\"\"\n",
    "    def parse_age(age_str):\n",
    "        if pd.isna(age_str):\n",
    "            return np.nan\n",
    "        \n",
    "        age_str = str(age_str).lower()\n",
    "        \n",
    "        if 'year' in age_str:\n",
    "            try:\n",
    "                return float(age_str.split()[0]) * 365\n",
    "            except:\n",
    "                return np.nan\n",
    "        elif 'month' in age_str:\n",
    "            try:\n",
    "                return float(age_str.split()[0]) * 30\n",
    "            except:\n",
    "                return np.nan\n",
    "        elif 'week' in age_str:\n",
    "            try:\n",
    "                return float(age_str.split()[0]) * 7\n",
    "            except:\n",
    "                return np.nan\n",
    "        elif 'day' in age_str:\n",
    "            try:\n",
    "                return float(age_str.split()[0])\n",
    "            except:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "            \n",
    "    return age_series.apply(parse_age)\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Animal types\n",
    "plt.subplot(2, 2, 1)\n",
    "animal_types.head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Animal Types')\n",
    "plt.xlabel('Animal Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Sex distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "sex_counts = explorer.data['Sex upon Outcome'].value_counts()\n",
    "sex_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Sex Distribution')\n",
    "plt.ylabel('')\n",
    "\n",
    "# Age analysis (convert to numeric)\n",
    "plt.subplot(2, 2, 3)\n",
    "age_data = convert_age_to_days(explorer.data['Age upon Outcome'])\n",
    "age_data = age_data[age_data.notna() & (age_data < 3650)]  \n",
    "plt.hist(age_data / 365, bins=50, alpha=0.7)\n",
    "plt.title('Age Distribution (Years)')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Outcome by animal type\n",
    "plt.subplot(2, 2, 4)\n",
    "outcome_by_animal = pd.crosstab(explorer.data['Animal Type'], explorer.data['Outcome Type'])\n",
    "outcome_by_animal.div(outcome_by_animal.sum(axis=1), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Outcome Distribution by Animal Type')\n",
    "plt.xlabel('Animal Type')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Generate Key Insights\n",
    "Summarize the key findings from our data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key insights from the data\n",
    "print(\"Key Insights\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_animals = len(explorer.data)\n",
    "\n",
    "# Outcome insights\n",
    "outcomes = explorer.data['Outcome Type'].value_counts()\n",
    "adoption_rate = (outcomes.get('Adoption', 0) / total_animals) * 100\n",
    "\n",
    "print(f\"Adoption Rate: {adoption_rate:.1f}%\")\n",
    "\n",
    "# Time insights\n",
    "try:\n",
    "    # Make sure DateTime is properly converted\n",
    "    if 'DateTime' in explorer.data.columns:\n",
    "        explorer.data['DateTime'] = pd.to_datetime(explorer.data['DateTime'], errors='coerce')\n",
    "        valid_dates = explorer.data['DateTime'].dropna()\n",
    "        \n",
    "        if len(valid_dates) > 0:\n",
    "            peak_hour = valid_dates.dt.hour.mode()\n",
    "            peak_month = valid_dates.dt.month.mode()\n",
    "            \n",
    "            if len(peak_hour) > 0:\n",
    "                print(f\"Peak Activity Hour: {peak_hour.iloc[0]}:00\")\n",
    "            if len(peak_month) > 0:\n",
    "                print(f\"Peak Activity Month: {peak_month.iloc[0]}\")\n",
    "        else:\n",
    "            print(\"⚠️  Could not analyze time patterns (no valid dates)\")\n",
    "    else:\n",
    "        print(\"⚠️  No DateTime column found for time analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Time analysis failed: {e}\")\n",
    "\n",
    "# Animal insights\n",
    "if 'Animal Type' in explorer.data.columns:\n",
    "    top_animal = explorer.data['Animal Type'].mode()\n",
    "    if len(top_animal) > 0:\n",
    "        print(f\"🐕 Most Common Animal: {top_animal.iloc[0]}\")\n",
    "\n",
    "if 'Breed' in explorer.data.columns:\n",
    "    top_breed = explorer.data['Breed'].mode()\n",
    "    if len(top_breed) > 0:\n",
    "        print(f\"Most Common Breed: {top_breed.iloc[0]}\")\n",
    "\n",
    "# Age insights \n",
    "try:\n",
    "    if 'Age upon Outcome' in explorer.data.columns:\n",
    "        age_days = convert_age_to_days(explorer.data['Age upon Outcome'])\n",
    "        valid_ages = age_days.dropna()\n",
    "        \n",
    "        if len(valid_ages) > 0:\n",
    "            avg_age_years = valid_ages.mean() / 365\n",
    "            print(f\"Average Age: {avg_age_years:.1f} years\")\n",
    "        else:\n",
    "            print(\"⚠️  Could not calculate average age (no valid age data)\")\n",
    "    else:\n",
    "        print(\"⚠️  No age data found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Age analysis failed: {e}\")\n",
    "\n",
    "print(\"\\nData Exploration Complete!\")\n",
    "print(\"Use these insights to inform your ML model development.\")\n",
    "\n",
    "# Save processed data for ML training\n",
    "print(\"\\nSaving processed data for ML training...\")\n",
    "try:\n",
    "    processed_data = explorer.data.copy()\n",
    "    \n",
    "    # Add age in days if we can calculate it\n",
    "    if 'Age upon Outcome' in processed_data.columns:\n",
    "        processed_data['age_in_days'] = convert_age_to_days(processed_data['Age upon Outcome'])\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    import os\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    \n",
    "    # Save processed data\n",
    "    processed_data.to_csv('../data/processed_animal_data.csv', index=False)\n",
    "    print(\"✅ Processed data saved to ../data/processed_animal_data.csv\")\n",
    "    print(f\"   Saved {len(processed_data):,} records with {len(processed_data.columns)} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save processed data: {e}\")\n",
    "    print(\"You can still proceed with ML training using the raw data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns in the data\n",
    "print(\"Temporal Patterns\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Convert DateTime to pandas datetime with better error handling\n",
    "print(\"Converting DateTime column...\")\n",
    "try:\n",
    "    # First, let's see what the DateTime column looks like\n",
    "    print(\"Sample DateTime values:\")\n",
    "    print(explorer.data['DateTime'].head())\n",
    "    print(f\"DateTime column type: {explorer.data['DateTime'].dtype}\")\n",
    "    \n",
    "    # Convert to datetime with error handling\n",
    "    explorer.data['DateTime'] = pd.to_datetime(explorer.data['DateTime'], errors='coerce')\n",
    "    \n",
    "    # Check for any conversion failures\n",
    "    null_count = explorer.data['DateTime'].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"Warning: {null_count} DateTime values could not be converted and were set to NaT\")\n",
    "    \n",
    "    # Remove rows with invalid dates for temporal analysis\n",
    "    valid_dates_mask = explorer.data['DateTime'].notna()\n",
    "    temp_data = explorer.data[valid_dates_mask].copy()\n",
    "    print(f\"Using {len(temp_data):,} records with valid dates for temporal analysis\")\n",
    "    \n",
    "    # Extract time components\n",
    "    temp_data['Year'] = temp_data['DateTime'].dt.year\n",
    "    temp_data['Month'] = temp_data['DateTime'].dt.month\n",
    "    temp_data['Hour'] = temp_data['DateTime'].dt.hour\n",
    "    temp_data['DayOfWeek'] = temp_data['DateTime'].dt.day_name()\n",
    "    \n",
    "    print(\"✅ DateTime conversion successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error processing DateTime: {e}\")\n",
    "    print(\"Skipping temporal analysis\")\n",
    "    # Create a simple plot instead\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.text(0.5, 0.5, 'Temporal analysis skipped due to DateTime conversion issues', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)\n",
    "    plt.title('Temporal Analysis - Conversion Error')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    temp_data = None\n",
    "\n",
    "# Only proceed with temporal analysis if we have valid data\n",
    "if temp_data is not None and len(temp_data) > 0:\n",
    "    # Yearly trends\n",
    "    yearly_outcomes = temp_data.groupby(['Year', 'Outcome Type']).size().unstack(fill_value=0)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Yearly trends\n",
    "    plt.subplot(2, 2, 1)\n",
    "    yearly_outcomes.plot(kind='line', ax=plt.gca())\n",
    "    plt.title('Yearly Outcome Trends')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Monthly patterns\n",
    "    plt.subplot(2, 2, 2)\n",
    "    monthly_counts = temp_data['Month'].value_counts().sort_index()\n",
    "    monthly_counts.plot(kind='bar')\n",
    "    plt.title('Monthly Distribution')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Hourly patterns\n",
    "    plt.subplot(2, 2, 3)\n",
    "    hourly_counts = temp_data['Hour'].value_counts().sort_index()\n",
    "    hourly_counts.plot(kind='line', marker='o')\n",
    "    plt.title('Hourly Distribution')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Day of week patterns\n",
    "    plt.subplot(2, 2, 4)\n",
    "    dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dow_counts = temp_data['DayOfWeek'].value_counts().reindex(dow_order)\n",
    "    dow_counts.plot(kind='bar')\n",
    "    plt.title('Day of Week Distribution')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Update the main data with the converted DateTime\n",
    "    explorer.data['DateTime'] = pd.to_datetime(explorer.data['DateTime'], errors='coerce')\n",
    "    explorer.data.loc[valid_dates_mask, 'Year'] = temp_data['Year']\n",
    "    explorer.data.loc[valid_dates_mask, 'Month'] = temp_data['Month']\n",
    "    explorer.data.loc[valid_dates_mask, 'Hour'] = temp_data['Hour']\n",
    "    explorer.data.loc[valid_dates_mask, 'DayOfWeek'] = temp_data['DayOfWeek']\n",
    "    \n",
    "    print(f\"✅ Temporal analysis complete using {len(temp_data):,} valid records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
